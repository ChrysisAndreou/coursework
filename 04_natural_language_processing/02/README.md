# Twitter Sentiment Classification with NLP

## Project Description

This project addresses the task of sentiment analysis on Twitter data, fulfilling the requirements of "Assignment 2: Twitter Sentiment Classification with NLP". The primary goal is to apply various NLP techniques for text pre-processing and feature engineering to train and evaluate sentiment classification models.

The project explores two distinct classification tasks:
1.  **Binary Classification:** Positive vs. Negative sentiment.
2.  **Three-Class Classification:** Positive vs. Negative vs. Neutral sentiment.

The focus is on comparing the performance of different feature representations, including traditional bag-of-words models, TF-IDF, dense word embeddings (Word2Vec and GloVe), and sentiment lexicon scores.

## Dataset

The project utilizes the **`tweet_eval`** sentiment dataset, loaded via the Hugging Face `datasets` library. This dataset contains tweets labeled with positive, negative, and neutral sentiments, split into training, validation, and test sets.

## Features Engineered

At least three different feature types were engineered and combined to analyze their impact on model performance:

*   **Bag of Words (BoW):** Term frequency counts with unigrams and bigrams.
*   **TF-IDF:** Term Frequency-Inverse Document Frequency vectorization with unigrams and bigrams.
*   **Word2Vec Embeddings:** Custom-trained 100-dimensional Word2Vec (Skip-gram) model. Tweet vectors are generated by averaging the embeddings of their constituent words.
*   **GloVe Embeddings:** Pre-trained 100-dimensional Twitter GloVe embeddings (`glove.twitter.27B.100d`). Tweet vectors are generated by averaging word embeddings.
*   **Sentiment Lexicon Features:** Sentiment scores (negative, neutral, positive, compound) generated using the VADER (Valence Aware Dictionary and sEntiment Reasoner) lexicon.
*   **Combined Features:** Various combinations of the above features (e.g., TF-IDF + GloVe + Lexicon) were created to leverage the strengths of sparse, dense, and sentiment-based representations.

## Models Evaluated

Several standard machine learning classifiers were trained and evaluated on the engineered features:

*   Logistic Regression
*   Linear Support Vector Classifier (LinearSVC)
*   Random Forest
*   Multinomial Naive Bayes
*   Voting Classifier (combining Logistic Regression and LinearSVC)

## Setup and Installation

### 1. Clone the Repository

```bash
git clone <repository-url>
cd <repository-directory>
```

### 2. Set Up Python Environment

It is recommended to use a virtual environment.

```bash
python -m venv nlp_env
source nlp_env/bin/activate  # On Windows, use `nlp_env\Scripts\activate`
```

### 3. Install Required Libraries

Install all necessary packages using pip:

```bash
pip install pandas numpy matplotlib seaborn
pip install datasets preprocessor nltk scikit-learn gensim vaderSentiment ekphrasis
pip install twokenize
```

### 4. Download GloVe Embeddings

The script uses pre-trained GloVe embeddings for Twitter.
1.  Download the embeddings from the [Stanford GloVe project page](https://nlp.stanford.edu/projects/glove/) (specifically, the `glove.twitter.27B.zip` file, 1.42 GB).
2.  Extract the downloaded zip file.
3.  Place the extracted folder, named `glove.twitter.27B`, in the root directory of this project.
4.  Ensure the file `glove.twitter.27B.100d.txt` is present inside this folder.

*Note: The script will still run without the GloVe embeddings but will skip any feature combinations that require them.*

## How to Run

To execute the entire pipeline from data pre-processing to model evaluation and result visualization, run the Python script from the root directory:

```bash
python code.py
```

The script will:
1.  Load and pre-process the TweetEval dataset.
2.  Engineer all the specified feature sets.
3.  Train and evaluate each model on every feature combination for both binary and three-class tasks.
4.  Print a detailed classification report for each experiment to the console.
5.  Display a final summary table comparing the F1-scores, training times, and resource usage across all experiments.
6.  Generate and display plots visualizing the model performance for both tasks.

## Project Structure

```
.
├── code.py                   # Main Python script with the complete pipeline.
├── jupyter.ipynb             # Jupyter Notebook version of the script.
├── html.html                 # HTML export of the Jupyter Notebook with outputs.
└── README.md                 # This README file.
```

## Summary of Results

The evaluation was performed on the test set using models trained with default hyperparameters.

### Key Findings

*   **Feature Combinations are Key:** For both binary and three-class tasks, combining diverse feature types (e.g., `TF-IDF + GloVe + Lexicon`) consistently outperformed models using a single feature type. This demonstrates the value of capturing semantic meaning (GloVe), term importance (TF-IDF), and explicit sentiment (VADER) simultaneously.
*   **Binary vs. Three-Class:** As expected, the binary classification task (Positive/Negative) achieved much higher performance (Macro F1-score up to **0.87**) compared to the three-class task (Macro F1-score up to **0.63**), highlighting the difficulty in distinguishing the neutral class.
*   **Best Performers:**
    *   For the **Binary task**, the `TF-IDF+GloVe+Lexicon` feature set paired with a **Voting Classifier** or **Logistic Regression** yielded the highest F1-scores.
    *   For the **Three-Class task**, the `GloVe+Lexicon` feature set with **Logistic Regression** performed best.
*   **Efficiency vs. Performance:** While complex combined features offered the best accuracy, they also required significantly more memory and training time. Simpler features like GloVe or TF-IDF alone provided strong baselines with much faster training.

## Challenges and Potential Improvements

*   **Challenges:**
    *   The quality of text pre-processing significantly impacts downstream performance.
    *   Combining sparse (TF-IDF) and dense (GloVe) feature matrices required careful handling.
    *   Training models on high-dimensional combined features was computationally more intensive.
*   **Potential Improvements:**
    *   **Hyperparameter Tuning:** Using techniques like Grid Search or Optuna would likely boost performance for all models.
    *   **Advanced Embeddings:** Exploring other pre-trained models like FastText or fine-tuning embeddings on the target dataset.
    *   **Deep Learning Models:** Implementing baseline deep learning models (e.g., LSTMs, CNNs) for comparison.
    *   **Error Analysis:** A detailed analysis of misclassified tweets could reveal patterns and guide further feature engineering or model selection.
